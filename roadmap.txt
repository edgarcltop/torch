- Try another solution by subclassing tensor and modifying __new__

Complications
- Okay, I think mutating torch.Tensor will work actually. This will be the completely exhaustive solution.
  And easy to fix: simply use the reload function to reload the torch namespace after. To go through all the functions,
  use the overridable ones first and crawl through those, then manually do the ignored ones that return Tensors.
  Won't take TOO long. Figure out how long it takes to do and undo; might be good to do this on import
  rather than on each function call. Figure out how long it takes. Actually: maybe just strip off
  the top level of the "torch" name, then put the right things back on. Perhaps that's the way. Okay
  so have the mutant versions along with the original versions defined in the module, then in the actual
  function can easily tack them on and off without too much trouble.
  The only trick will be designing the decorator:
    - Already made the lil hack for printing withou infinite recursion
    - Check if output is a tensor, with a new grad_fn (need some kind of system for this). If so, save it.
    - Let's additionally save any input tensors, and new tensors created inside the network.
      The logic will be: mark a tensor as input when it comes in, and any outputs from a function
      with at least one input tensor as input will also be marked as input. For each tensor argument to
      a function not marked as input, it'll be marked as "new_tensor" and saved, and resulting tensors
      will be marked as "internal_origin", until they run into a tensor marked "input tensor".
      How about: origin: input or origin: internal. Okay can actually decorate EVERY function. Nice.
      So all we have to do is, for these internally generated tensors, can get the literal function call and
      annotate that way :] Prob good to include non-differentiable operations too. Can still
      track the graph using the barcode trick; if tensor output doesn't have a barcode, stamp it with one and track it.
      This suffices to fully generate the graph; store the edges as barcodes, and have them serve as pointers
      to their corresponding records. Figure out how long this takes
      - Nasty complication: anything created during the mutation keeps all the mutated definitions.
        Either don't touch them again, or have a way of restoring them.
      - Adopted solution of simply deepcopying user input, then mutate all tensors in the input
      before passing into the forward pass. This keeps them unchanged, except for wrapping the functions
      the way we'd like.
- Have an option for setting a random seed. This will fully cover cases where there's randomness in
  the network.
  - Add module pre-hook for marking tensors with the module they're in.
- Need a function for finding tensors in any potentially-nested input so they can be tagged as input.
  Maybe just check if it's iterable and do some nesting, and go through any non-underscore methods?
  If it's hidden but also important it's their own fault.
- Work on graph last once everything else is working since it doesn't depend on the rest.
- Label the exhaustive/function graph based on the actual functions, no need for the grad_fn workaround.
- Prob just ask that the user not run anything in parallel so as to avoid weird execution problems. Can
  also just have a warning that says "don't run in parallel" and then have a flag to disable the warning.
  This is a good idea, because it'll be a pain to debug if the user runs in parallel and gets weird results.
- Dealing with fact that the final "nice" layer names might not be the same as the ones generated on the
  fly, so how do we know whether to save them or not; can do two forward passes; during the first one,
  generate a dictionary mapping from initial names to final nice names (saving nothing), and during
  the second pass, use that dictionary to know which activations to actually save (only slightly wasteful).
  Best way to mark: maybe annotate each tensor with how many computional steps have happened since each
  input Tensor.
- The relabeling is probably desirable, no guarantee of a nice ordering during the forward pass.
  Do a post-hoc topological sort after the fact.
- Need a function to recursively crawl the namespace of the input and find any tensors so they can be marked as input.
  This is not needed for internally generated because they'll run into a Tensor operation at some point and get
  marked that way.
- Let's decide to only track TENSORS. Requiring everything else is simply impossible.
- Which operations to track? We want to avoid going into the ugly guts of modules,
  and have operations correspond to actually human-relevant ones.
- Maybe we don't need to tag the C functions since the Python functions themselves call these functions?
  REVERSIBLY remove these from the big list and see if anything is lost.


Cleanup Tasks:
- Remove the ignore functions that are pointless. This will require one tedious pass.

Graph visuals:
- Indicate nodes that have parameters (bold?)
- Indicate nodes that are purely internally generated and haven't seen the input (maybe a light gray)?
-

Naming considerations:
- Layer, operation, module, function... get clear and consistent on this.

Performance considerations
- Time: how many forward passes? Any network copying?
- Does everything work when you shift to parallel processing?
- Guaranteeing network unchanged: maybe have a network cleanup function, wrap in a try-except block? Just
    need a way of clearing the forward hooks. This is easy, just keep a list of the handles in the module,
    clear each when you clean up, then delete the list. Model should be unchanged.

Testing:
- Test with parallel, GPU, PyTorch Lightning, etc.



TO DO:
- Add more annotation to the tensor log
- Line up the modules and the tensors, they should annotate each other. Add the module pre-hooks.
- Once the output for both is nice, add functions to clean up the annotations and graph
- Remove redundant nodes (e.g. the double relus; see if the C stuff can be un-annotated without problems)
- Do the graph visualization stuff
- Test for multiple inputs.
- Organize code better.
- Do the wish list:
    - sub-class support
    - Add random seed
    - Remove functions from the ignore list that aren't needed. Take a few hours for this on a slow day.
    - Add warning if it's a parallel thread.
- Test on a bunch of REAL models once it's the way you like it. And do some timing to make sure it's no slower.
