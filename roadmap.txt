Wishlist:
- Horizontally sort branching layers in the graph visual based on execution order (this is quite hard
  to do along with clusters)
- Deal with graph clutter, arrows running over stuff, etc.
- Aggregate multiple add/multiply/etc. operations into a single one for conciseness
- Annotate pass numbers for higher level modules too
- Have nodes in the pretty dict be their own nice class.
- Ditch the annoying nested dicts and multiple rounds of transformation, make everything nice and object-oriented;
  along with better way of unmutating things after the fact. One tensor log is enough. And it should have enough
  for all plotting and not require references to modules, these should simply be tagged. The prettify
  TensorLog function should be a method that just removes extraneous interim information that was useful
  when generating the graph but not otherwise; it should still contain all relevant information for making the
  figures, etc. And it should be inspectable at any time for debugging purposes without it yelling.
- Track all the module information in a nicer way as you go, have nicer names for things. Make everything as
  transparent as possible with better debugging tools.
- Better way of handling the identity functions
- Ditch the c-bindings, don't tag them. The top-level python functions are plenty.
- Refactor complex functions to be nicer.
- Add a "debug/validation" mode that exhaustively saves all arguments.
- Consistent handling of module "passes"
- The object-based refactor should go one step at a time to make sure it all works.
- Do more stuff on the fly, less post-processing. Instead of your "3-4 magic functions", just two:
  one to get the graph object (which can be plotted at will), and then one to get any activations.
- Have it save the operation numbers as part of that object so they can be easily added after.
- Some kind of API that separates the saved activations, and the full graph, for easy inspection.
- Maybe: indexing by default pulls out anything, activation or no, but can iterate through saved activations,
  get keys for saved activations, etc.
- The tensor log is defined arond one pass for one input.
- Maybe a final function that replaces raw barcodes with the new ones.
- Get clear on the different "states" of the tensorlog: on-the-fly kinda raw one, validation one, processed one,
  pretty user-facing one.
- How to handle multiple instances/passes of the same module; the entry stamp concept is a good one.
- Save all func args and kwargs by default, as long as activations are also saved. Can be pointers to the arg parameters;
  no need for separate validation mode, if tensor is saved then save these too.
- Also a function to clear activations while keeping the graph. Have nicer syntax for indicating this. Actually
  once this is present, redoing the feedforward sweep is very easy.
- Basically, the abstraction is a saved full graph, from which you can attach activations with another feedforward
  pass.
- Also a nice function to rigorously clear all activations so they don't clutter things.

To Do
- Make sure unmutate pytorch works, it's being weird.
- Do some profiling to see which steps are and aren't necessary.
- Inspect cornet_s closely to make sure recurrent stuff works. It's quite close, just check how it
  does the layer correspondence
- Fix topological sort for extended internally generated tensors
- Propagate nested modules to internally generated tensors
- Make topological sort follow operation order; if there's branching, then the order of the first
  nodes in the branch determine the rest.
- Make sure that the rolled view plays nice with the module view.
- Email Hebart et al.
- Validation script, unit tests.
- Lil photo gallery
- Write out a schematic of the code, go back through and make it pretty; remove worthless stuff.
- Make the demos, and a quick, nice readme, CoLab, twitter thread
- Roll out by Wednesday
- Making it work with parallelism probably too hard (tracking the tensor module depends greatly on tensor state).
Just stipulate NOT to do this.
- Go through and make sure all naming etc. is consistent
- Do profiling and figure out the slow parts
- Add more options for the graph visualizer
- Code the top-level user interface; optimize default options for speed and most common use cases.
- Code the validator script, and all testing; the unit tests can employ this.
- Package into a proper GitHub module
- Make sure cleanup works properly, all tensors, modules, parameters, etc.
- Figure out easiest way to install graphviz for users, this could be a pain point.
- Make the CoLab notebook, visuals, twitter thread.
- Name it
- Add better support for jupyter visualizations
- Email Hebart folks
- The stumbling blocks are the graph operations.

A new complication: a computational bottom-level module need not be a bottom-level module at the level of indexing
a module object. Have the convention of user-facing addresses be with respect to the indexing, but
under the hood "bottom-level" means that a module only contains one tensor operation.

Logic for my own reference:
- The whole challenge, the one and only main challenge, is keeping track of which functions are entered and left,
  and which modules are entered and left. That's it.



Justification for it:
- Module level can be MISLEADING! Module structure does not directly inform the control flow. 
- Thus, handling of recurrent stuff is clunky.
- Can't handle function calls. 
- And can't handle inner branching logic. 
- Branching handled clumsily. 
- MAYBE don't index layers by modules? Since they lack parameters, there's no guarantee they'll correspond to meaningful computational steps. 


Tensors, layers, nodes, modules

Data Structure Fields:


Benchmarks to Meet Before Release:
- Works on all models that Colin mentioned
- Doesn't add TOO much time to forward pass
- Works on different Torch versions, is stable
- Make the code well-documented, pretty
- Have a Colab notebook that shows it off
- Good, and realistic pictures showing what it does
- Crisp twitter thread.
- Try different architectures: language, video, LSTM, transformers, RL.
- Function that returns an informative but exhaustive graph representation for the user

Graph Display: unrolled
layer_name (pass 1/3)
@layer_address
dtype tensor_dimensions (tensor_filesize)
param_dimensions

Graph Display: rolled
Same as unrolled, but edges annotated with the pass numbers in a nice way


Nomenclature:
- Module: PyTorch object type
- Function: anything that operates on a tensor and returns a tensor
- Operation: any time a module or function does something on a tensor
- Layer: a computational step that could meaningfully count as being stable across recurrent passes.
- Default ontology is tensors that are the results of operations; everything else is packaging.
  The tensor will however know what modules it came out of.
- Modules are always indexed by their address (the double names are very confusing, don't do it)

Layer Semantics:
- What counts as a layer? Determined in this order, each taking precedence over the next:
    1. A lowest-level module, 'bottom-level-module'
    2. A group of parameters, even if unattached to a module, 'param-function'
    3. Function calls intervening between layers that are stable across passes, 'no-param-function'
       This third one is the trickiest to handle.

- Barcode types
  - tensor/operation (each tensor comes out of an operation)
  - layer
  - module
  - parameter

Ways of individuating layers (= "same" operation across passes)
- Same module
- Same parameter group
- Same position in the outermost loop
- Track the tensor barcodes for each of these so they can be independently indexed

Non-Urgent Wishlist
- Subclass support
- Support chooosing layers by their nested address
- Have the rolled-up graph display option


Overall logic: saving all functions is the baseline and the first thing it does, pulling out modules
    if desired after is done post-hoc based on stuff saved during the forward pass.

History Dict:
tensor_nums_to_save: indicates which tensor numbers to save.
tensor_num: counter that keeps track of how many tensors have been made
tensor:
  xray_barcode
  xray_tensor_num
  xray_containing_modules_nested
  xray_containing_module
  xray_entered_module
  xray_last_module_seen
  xray_last_module_seen_address
  xray_tensor_args
  xray_tensor_params
  xray_tensor_activations
  xray_tensor_size
tensor_log: same as tensor, but also saving the tensor itself, and the args, kwargs, and params if desired.

Module barcode can simply be the address

Distinction between passes through a module and passes through a set of parameters. The former is just
for addressing

Module:
xray_module_address
xray_module_type
xray_is_bottom_level_module
xray_module_pass_number

User-Facing Syntax:
- Default is to only look at "bottom-level" modules and functions.
- Still allow indexing repeated modules by pass number, but this doesn't factor into the visuals,
  and is separate from their computational pass number.
- Okay, two modes: exhaustive and modules_only.
#TODO: figure out the most natural syntax for handling modules versus functions that isn't too bulky.
#      and figure out whether it's worth labeling functions in nested format too, like the torch version does.
#      Just add a counter to the tensors that keeps track of how many functions have been called since entering a
#      module. Eh the override is too confusing and they can do that themselves if fussy.
#      Maybe dump functions-nested, just no good way to do it... maybe a prefix mod_ or fn_? But it's ugly extra ink.
#      Combine simplicity and flexibility however you can. Do everything except this first, just make all the information
#      present and easy to get. Trade off simplicity and flexibility. ACTUALLY maybe the best
#      way is to have separate which_funcs, which_modules arguments. Figure it out once all is working.
- And for which_layers, maybe have these as keywords too.
- This determines the labeling scheme,
  and the behavior of 'all' if supplied as argument for which_layers.
- But to look at higher-level modules provide some override behaviors: mark the graph and the output
  with their module location and whether they're the output of a module, allow user to specify this in the
  layer list, and also have an option to list all of these in the list_layers function. Main thing is to
  annotate tensors with the modules that they have left, this is sufficient to do all this.
- Allow "input" and "output" as numbered keywords for indexing (e.g., output1, output2, etc.)
- Maybe the returned data structure should be indexable by either index or layer name? And multiple layer names?
- Maybe have both sequential module mode, and nested module mode?
- Returned dictionary should be topologically sorted and addressed in the desired manner.

Inner syntax:
- Separate barcodes for: operations

Testing:
- Have a function that reconstructs the whole forward pass from each saved input, and checks whether it matches
  the true output. This can be done by saving the arguments, checking the correspondence between node outputs and
  next node inputs, and running the graph this way. Do this for several different arguments.

Cleanup:
- Cleanup the modules and params
- Input tensors are fine since they're deepcopied.
- Tensors created internally are detached first.

Graph Functions:
- Have a bunch of discrete operations that can be strung together.
- Operations:
    - strip_non_modules: removes all nodes that aren't the output of a module
    - expand_identity_functions: adds nodes for tensors that went through identity functions, for fidelity to the graph
    - add_input_nodes: Add nodes for the inputs
    - add_output_nodes: Add nodes for the outputs
    - add_parent_fields: Add fields for the node parents so it can be traversed either way
    - strip_irrelevant_nodes: Remove nodes that don't affect the output of the model
    - annotate_node_names: Go through and annotate the nodes with the nice naming system, closer to what user will see.
    - roll_graph: Roll together recurrent operations, annotating how many passes they see (maybe do this per arrow)
    - topological_sort_nodes: mark each node with its topological sort number
    - connect_node_arguments: check which nodes outputs correspond to which node inputs (this is only
      used for testing); goal is to find where that tensor appears in the arguments to the next function,
      and then funnel it there. Just need some kind of recipe for doing this.
    - Goal is to get ALL the information needed to either view the graph, fetch information, or return
      information to the user (needn't be the final "pretty" version, this is a post-processed version
      after the forward sweep).

Package Organization:
- torch_xray: The user-facing functions (this can import from everything, nothing imports from this)
- graphs: For processing and visualizing the graphs
- xray_utils: Random utility functions (everything imports from here)
- torch_func_handling: For dealing with mutating the torch functions; imports only from xray_utils
- tensor_logging: Any tensor-logging operations; AKA the heart of the package.
- testing.py: My own internal testing functions for validating a model.



Past Log (for reference)
- Okay, I think mutating torch.Tensor will work actually. This will be the completely exhaustive solution.
  And easy to fix: simply use the reload function to reload the torch namespace after. To go through all the functions,
  use the overridable ones first and crawl through those, then manually do the ignored ones that return Tensors.
  Won't take TOO long. Figure out how long it takes to do and undo; might be good to do this on import
  rather than on each function call. Figure out how long it takes. Actually: maybe just strip off
  the top level of the "torch" name, then put the right things back on. Perhaps that's the way. Okay
  so have the mutant versions along with the original versions defined in the module, then in the actual
  function can easily tack them on and off without too much trouble.
  The only trick will be designing the decorator:
    - Already made the lil hack for printing withou infinite recursion
    - Check if output is a tensor, with a new grad_fn (need some kind of system for this). If so, save it.
    - Let's additionally save any input tensors, and new tensors created inside the network.
      The logic will be: mark a tensor as input when it comes in, and any outputs from a function
      with at least one input tensor as input will also be marked as input. For each tensor argument to
      a function not marked as input, it'll be marked as "new_tensor" and saved, and resulting tensors
      will be marked as "internal_origin", until they run into a tensor marked "input tensor".
      How about: origin: input or origin: internal. Okay can actually decorate EVERY function. Nice.
      So all we have to do is, for these internally generated tensors, can get the literal function call and
      annotate that way :] Prob good to include non-differentiable operations too. Can still
      track the graph using the barcode trick; if tensor output doesn't have a barcode, stamp it with one and track it.
      This suffices to fully generate the graph; store the edges as barcodes, and have them serve as pointers
      to their corresponding records. Figure out how long this takes
      - Nasty complication: anything created during the mutation keeps all the mutated definitions.
        Either don't touch them again, or have a way of restoring them.
      - Adopted solution of simply deepcopying user input, then mutate all tensors in the input
      before passing into the forward pass. This keeps them unchanged, except for wrapping the functions
      the way we'd like.
- Have it save the parameters too, if you save a layer. Completeness. Maybe a flag not to. But at least save the size of the params.
- Have an option for setting a random seed. This will fully cover cases where there's randomness in
  the network.
  - Add module pre-hook for marking tensors with the module they're in.
- Need a function for finding tensors in any potentially-nested input so they can be tagged as input.
  Maybe just check if it's iterable and do some nesting, and go through any non-underscore methods?
  If it's hidden but also important it's their own fault.
- Work on graph last once everything else is working since it doesn't depend on the rest.
- Label the exhaustive/function graph based on the actual functions, no need for the grad_fn workaround.
- Prob just ask that the user not run anything in parallel so as to avoid weird execution problems. Can
  also just have a warning that says "don't run in parallel" and then have a flag to disable the warning.
  This is a good idea, because it'll be a pain to debug if the user runs in parallel and gets weird results.
- Dealing with fact that the final "nice" layer names might not be the same as the ones generated on the
  fly, so how do we know whether to save them or not; can do two forward passes; during the first one,
  generate a dictionary mapping from initial names to final nice names (saving nothing), and during
  the second pass, use that dictionary to know which activations to actually save (only slightly wasteful).
  Best way to mark: maybe annotate each tensor with how many computional steps have happened since each
  input Tensor.
- The relabeling is probably desirable, no guarantee of a nice ordering during the forward pass.
  Do a post-hoc topological sort after the fact.
- Need a function to recursively crawl the namespace of the input and find any tensors so they can be marked as input.
  This is not needed for internally generated because they'll run into a Tensor operation at some point and get
  marked that way.
- Let's decide to only track TENSORS. Requiring everything else is simply impossible.
- Which operations to track? We want to avoid going into the ugly guts of modules,
  and have operations correspond to actually human-relevant ones.
- Maybe we don't need to tag the C functions since the Python functions themselves call these functions?
  REVERSIBLY remove these from the big list and see if anything is lost.
- Idea: simplify the innards of it: if there's going to be two passes and graph cleanup anyways,
  no need to save all the stuff on the first pass. Save the minimal amount.
- Make sure same random seed for both passes (a "probe" pass and a "save" pass) to guarantee same output
- Add function to check if being run in parallel and stop execution. Add a manual override if user is okay
  with potential errors.
- I THINK two forward passes is essential: if numbering is based on a topological sort that can't be known until
  the pass is done, then you really need to do a pass first. This favors doing minimal labeling during
  the initial pass, and only doing the full labeling afterward. Individuate things by inner barcodes and an operation counter,
  and then after the probe pass create a mapping between the operation counter and these labels to figure out whether to
  save or not. But do save any information that can't be recovered after: for example, the size of the tensors
  and parameters, etc.
- For the wrapped representation of the function view, stipulate that all parameter objects have to have been
  seen the same number of times in order to count as the same object. Else don't wrap it.
- Unwrapped version of graph is default, wrapped is extra.
- Start being more minimal in what's saved.
- No need for the extra module record: the tensor record suffices, along with a tag specifying if a tensor just
  left a bottom-level module. But do crawl through and hook with their addresses.
- Allow getting the output any module, not just the top-level ones, by specifying the address. That is,
  the default will be the bottom-level modules, but the user can override this if desired.
- Doesn't make sense to allow specifying addresses for functions since they can't be retrieved in that way,
  but can follow the Torch syntax if people really want... not a critical feature.
- What to tag on the tensor itself versus the log: tag stuff to the tensor itself if it's required for
  any on-the-fly decisions about computations and so on.
- Also include inputs and outputs in the tensor log for completeness: marked for each tensor, input1, input2, output1, output2, etc.
- Add input tensors to the log, too.
- Okay, individuate tensors by a single counter. This should be fool-proof as long as it executes everything the same order
 (if random seed and no parallel processing this should be fine).
- Have way of letting user specify the layer string format. Why not.
- Mostly link up the tensor data and the tensor log data, with a function to easily update them.
- Get the parameter sizes too, along with total file size.

Modules Stuff:
- Attributes: address and whether they're a bottom-level module
- Hooks: tacking stuff onto a tensor on the way in or out.
- Can easily generate the module-level graph from the tensor-level graph by just pruning in-between nodes.

Tensor stuff:
- Each tensor has a barcode, this links it to the tensor log, and also the tensors to each other.


Cleanup Tasks:
- Remove the ignore functions that are pointless. This will require one tedious pass.

Graph visuals:
- Indicate nodes that have parameters (bold?)
- Indicate nodes that are purely internally generated and haven't seen the input (maybe a light gray)?
-

Naming considerations:
- Layer, operation, module, function... get clear and consistent on this.

Performance considerations
- Time: how many forward passes? Any network copying?
- Does everything work when you shift to parallel processing?
- Guaranteeing network unchanged: maybe have a network cleanup function, wrap in a try-except block? Just
    need a way of clearing the forward hooks. This is easy, just keep a list of the handles in the module,
    clear each when you clean up, then delete the list. Model should be unchanged.

Testing:
- Test with parallel, GPU, PyTorch Lightning, etc.




TO DO:
- Add more annotation to the tensor log
- Line up the modules and the tensors, they should annotate each other. Add the module pre-hooks.
- Once the output for both is nice, add functions to clean up the annotations and graph
- Remove redundant nodes (e.g. the double relus; see if the C stuff can be un-annotated without problems)
- Do the graph visualization stuff
- Test for multiple inputs.
- Organize code better.
- Do the wish list:
    - sub-class support
    - Add random seed
    - Remove functions from the ignore list that aren't needed. Take a few hours for this on a slow day.
    - Add warning if it's a parallel thread.
- Test on a bunch of REAL models once it's the way you like it. And do some timing to make sure it's no slower.
- Give access to a few people who want to try it.
- Add option to save all function arguments too; this should make it possible to fully reconstruct the output
  as a sanity check (tester function: crawl back from the output, keep crawling back and evaluating the output
  from the saved tensors, functions, arguments). Do this for many networks, at least the full torch
  model zoo.
- Add functionality for specifying desired layers based on either network architecture or based
  on the way I'm numbering it
- Fix the decorator to inherit metadata from the function, clean things up.
