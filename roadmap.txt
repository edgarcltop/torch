- Try another solution by subclassing tensor and modifying __new__

Complications
- Okay, I think mutating torch.Tensor will work actually. This will be the completely exhaustive solution.
  And easy to fix: simply use the reload function to reload the torch namespace after. To go through all the functions,
  use the overridable ones first and crawl through those, then manually do the ignored ones that return Tensors.
  Won't take TOO long. Figure out how long it takes to do and undo; might be good to do this on import
  rather than on each function call. Figure out how long it takes. Actually: maybe just strip off
  the top level of the "torch" name, then put the right things back on. Perhaps that's the way. Okay
  so have the mutant versions along with the original versions defined in the module, then in the actual
  function can easily tack them on and off without too much trouble.
  The only trick will be designing the decorator:
    - Already made the lil hack for printing withou infinite recursion
    - Check if output is a tensor, with a new grad_fn (need some kind of system for this). If so, save it.
    - Let's additionally save any input tensors, and new tensors created inside the network.
      The logic will be: mark a tensor as input when it comes in, and any outputs from a function
      with at least one input tensor as input will also be marked as input. For each tensor argument to
      a function not marked as input, it'll be marked as "new_tensor" and saved, and resulting tensors
      will be marked as "internal_origin", until they run into a tensor marked "input tensor".
      How about: origin: input or origin: internal. Okay can actually decorate EVERY function. Nice.
      So all we have to do is, for these internally generated tensors, can get the literal function call and
      annotate that way :] Prob good to include non-differentiable operations too. Can still
      track the graph using the barcode trick; if tensor output doesn't have a barcode, stamp it with one and track it.
      This suffices to fully generate the graph; store the edges as barcodes, and have them serve as pointers
      to their corresponding records. Figure out how long this takes
      - Nasty complication: anything created during the mutation keeps all the mutated definitions.
        Either don't touch them again, or have a way of restoring them.
      - Adopted solution of simply deepcopying user input, then mutate all tensors in the input
      before passing into the forward pass. This keeps them unchanged, except for wrapping the functions
      the way we'd like.
- Have an option for setting a random seed. This will fully cover cases where there's randomness in
  the network.
  - Add module pre-hook for marking tensors with the module they're in.
- Need a function for finding tensors in any potentially-nested input so they can be tagged as input.
  Maybe just check if it's iterable and do some nesting, and go through any non-underscore methods?
  If it's hidden but also important it's their own fault.
- Work on graph last once everything else is working since it doesn't depend on the rest.
- Label the exhaustive/function graph based on the actual functions, no need for the grad_fn workaround.
- Prob just ask that the user not run anything in parallel so as to avoid weird execution problems. Can
  also just have a warning that says "don't run in parallel" and then have a flag to disable the warning.
  This is a good idea, because it'll be a pain to debug if the user runs in parallel and gets weird results.
- Dealing with fact that the final "nice" layer names might not be the same as the ones generated on the
  fly, so how do we know whether to save them or not; can do two forward passes; during the first one,
  generate a dictionary mapping from initial names to final nice names (saving nothing), and during
  the second pass, use that dictionary to know which activations to actually save (only slightly wasteful).
  Best way to mark: maybe annotate each tensor with how many computional steps have happened since each
  input Tensor.
- The relabeling is probably desirable, no guarantee of a nice ordering during the forward pass.
  Do a post-hoc topological sort after the fact.
- Need a function to recursively crawl the namespace of the input and find any tensors so they can be marked as input.
  This is not needed for internally generated because they'll run into a Tensor operation at some point and get
  marked that way.
- Let's decide to only track TENSORS. Requiring everything else is simply impossible.
- Which operations to track? We want to avoid going into the ugly guts of modules,
  and have operations correspond to actually human-relevant ones.
- Maybe we don't need to tag the C functions since the Python functions themselves call these functions?
  REVERSIBLY remove these from the big list and see if anything is lost.
- Idea: simplify the innards of it: if there's going to be two passes and graph cleanup anyways,
  no need to save all the stuff on the first pass. Save the minimal amount.
- Make sure same random seed for both passes (a "probe" pass and a "save" pass) to guarantee same output
- Add function to check if being run in parallel and stop execution. Add a manual override if user is okay
  with potential errors.
- I THINK two forward passes is essential: if numbering is based on a topological sort that can't be known until
  the pass is done, then you really need to do a pass first. This favors doing minimal labeling during
  the initial pass, and only doing the full labeling afterward. Individuate things by inner barcodes and an operation counter,
  and then after the probe pass create a mapping between the operation counter and these labels to figure out whether to
  save or not. But do save any information that can't be recovered after: for example, the size of the tensors
  and parameters, etc.
- For the wrapped representation of the function view, stipulate that all parameter objects have to have been
  seen the same number of times in order to count as the same object. Else don't wrap it.
- Unwrapped version of graph is default, wrapped is extra.
- Start being more minimal in what's saved.
- No need for the extra module record: the tensor record suffices, along with a tag specifying if a tensor just
  left a bottom-level module. But do crawl through and hook with their addresses.
- Allow getting the output any module, not just the top-level ones, by specifying the address. That is,
  the default will be the bottom-level modules, but the user can override this if desired.
- Doesn't make sense to allow specifying addresses for functions since they can't be retrieved in that way,
  but can follow the Torch syntax if people really want... not a critical feature.
- What to tag on the tensor itself versus the log: tag stuff to the tensor itself if it's required for
  any on-the-fly decisions about computations and so on.

Modules Stuff:
- Attributes: address and whether they're a bottom-level module
- Hooks: tacking stuff onto a tensor on the way in or out.
- Can easily generate the module-level graph from the tensor-level graph by just pruning in-between nodes.

Tensor stuff:
- Each tensor has a barcode, this links it to the tensor log, and also the tensors to each other.


Cleanup Tasks:
- Remove the ignore functions that are pointless. This will require one tedious pass.

Graph visuals:
- Indicate nodes that have parameters (bold?)
- Indicate nodes that are purely internally generated and haven't seen the input (maybe a light gray)?
-

Naming considerations:
- Layer, operation, module, function... get clear and consistent on this.

Performance considerations
- Time: how many forward passes? Any network copying?
- Does everything work when you shift to parallel processing?
- Guaranteeing network unchanged: maybe have a network cleanup function, wrap in a try-except block? Just
    need a way of clearing the forward hooks. This is easy, just keep a list of the handles in the module,
    clear each when you clean up, then delete the list. Model should be unchanged.

Testing:
- Test with parallel, GPU, PyTorch Lightning, etc.




TO DO:
- Add more annotation to the tensor log
- Line up the modules and the tensors, they should annotate each other. Add the module pre-hooks.
- Once the output for both is nice, add functions to clean up the annotations and graph
- Remove redundant nodes (e.g. the double relus; see if the C stuff can be un-annotated without problems)
- Do the graph visualization stuff
- Test for multiple inputs.
- Organize code better.
- Do the wish list:
    - sub-class support
    - Add random seed
    - Remove functions from the ignore list that aren't needed. Take a few hours for this on a slow day.
    - Add warning if it's a parallel thread.
- Test on a bunch of REAL models once it's the way you like it. And do some timing to make sure it's no slower.
- Give access to a few people who want to try it.
- Add option to save all function arguments too; this should make it possible to fully reconstruct the output
  as a sanity check (tester function: crawl back from the output, keep crawling back and evaluating the output
  from the saved tensors, functions, arguments). Do this for many networks, at least the full torch
  model zoo.
- Add functionality for specifying desired layers based on either network architecture or based
  on the way I'm numbering it
- Fix the decorator to inherit metadata from the function, clean things up.
